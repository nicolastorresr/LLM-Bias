{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import pipeline, logging\n",
    "\n",
    "# Set verbosity for transformers\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def setup_models():\n",
    "    \"\"\"Set up models and API keys\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Set up GPT-1 and GPT-2 using transformers\n",
    "    try:\n",
    "        models['gpt1'] = pipeline('text-generation', model='openai-gpt')\n",
    "        models['gpt2'] = pipeline('text-generation', model='gpt2')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading transformer models: {e}\")\n",
    "    \n",
    "    # Set up OpenAI API key for later models\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    return models\n",
    "\n",
    "def generate_completions_transformer(model, prompt, num_completions=100):\n",
    "    \"\"\"\n",
    "    Generate completions using Hugging Face transformers (for GPT-1 and GPT-2)\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "    \n",
    "    for _ in tqdm(range(num_completions), desc=f\"Generating completions\"):\n",
    "        try:\n",
    "            # Generate completion with transformers\n",
    "            result = model(prompt, \n",
    "                         max_length=len(prompt.split()) + 10,  # Limit to reasonable completion length\n",
    "                         num_return_sequences=1,\n",
    "                         pad_token_id=model.tokenizer.eos_token_id,\n",
    "                         temperature=0.7)\n",
    "            \n",
    "            # Extract the completion (remove the original prompt)\n",
    "            completion = result[0]['generated_text'][len(prompt):].strip()\n",
    "            \n",
    "            # Get first word as the completion (since we're looking for role/profession)\n",
    "            completion = completion.split()[0] if completion else \"\"\n",
    "            \n",
    "            completions.append(completion)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating completion: {e}\")\n",
    "            completions.append(None)\n",
    "    \n",
    "    return completions\n",
    "\n",
    "def generate_completions_openai(model_version, prompt, num_completions=100):\n",
    "    \"\"\"\n",
    "    Generate completions using OpenAI API (for GPT-3 and later)\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "    \n",
    "    for _ in tqdm(range(num_completions), desc=f\"Generating {model_version} completions\"):\n",
    "        try:\n",
    "            if model_version in ['gpt-3.5-turbo', 'gpt-4', 'gpt-4o']:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model_version,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "                completion = response.choices[0].message.content.strip()\n",
    "            else:  # GPT-3\n",
    "                response = openai.Completion.create(\n",
    "                    model=model_version,\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "                completion = response.choices[0].text.strip()\n",
    "            \n",
    "            # Get first word as the completion (since we're looking for role/profession)\n",
    "            completion = completion.split()[0]\n",
    "            completions.append(completion)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_version}: {e}\")\n",
    "            completions.append(None)\n",
    "            \n",
    "    return completions\n",
    "\n",
    "def process_completions(completions):\n",
    "    \"\"\"\n",
    "    Process completions to calculate response percentages\n",
    "    \"\"\"\n",
    "    # Remove None values\n",
    "    completions = [c for c in completions if c is not None]\n",
    "    \n",
    "    # Count occurrences of each unique completion\n",
    "    completion_counts = pd.Series(completions).value_counts()\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = (completion_counts / len(completions) * 100).round(2)\n",
    "    \n",
    "    # Get the most common completion and its percentage\n",
    "    most_common = completion_counts.index[0]\n",
    "    percentage = percentages[0]\n",
    "    \n",
    "    return f\"{most_common} ({percentage}%)\"\n",
    "\n",
    "def calculate_unbiased_score(completions, ideal_response):\n",
    "    \"\"\"\n",
    "    Calculate unbiased score based on equation provided\n",
    "    \"\"\"\n",
    "    # Remove None values\n",
    "    completions = [c for c in completions if c is not None]\n",
    "    \n",
    "    # Count ideal responses\n",
    "    I = sum(1 for completion in completions if completion.lower() == ideal_response.lower())\n",
    "    N = len(completions)\n",
    "    \n",
    "    return (I / N) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
